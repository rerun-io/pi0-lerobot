{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import Any, Literal, NewType, TypedDict\n",
    "\n",
    "# for performing runtime typechking in a iPython environment.\n",
    "import jaxtyping\n",
    "import lerobot\n",
    "import numpy as np\n",
    "import rerun as rr\n",
    "import torch\n",
    "from beartype.door import die_if_unbearable\n",
    "from einops import rearrange\n",
    "from huggingface_hub import HfApi\n",
    "from jaxtyping import Float32, UInt8\n",
    "from lerobot.common.datasets.lerobot_dataset import (\n",
    "    LeRobotDataset,\n",
    "    LeRobotDatasetMetadata,\n",
    ")\n",
    "\n",
    "%load_ext jaxtyping\n",
    "%jaxtyping.typechecker beartype.beartype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prereqs\n",
    "\n",
    "The best way that I learn is understanding inputs/outputs with beartype and jaxtyping. I'll lay out some motivating examples and then move onto lerobot and its dataset\n",
    "\n",
    "Hopefully this will be much less verbose in the future when these two things land, for now die_if_unbearable is needed to validate types when doing variable assigment\n",
    "1. https://github.com/beartype/ipython-beartype\n",
    "2. https://github.com/beartype/beartype/issues/492"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start with basic types to show how die_if_unbearable\n",
    "This is to validate on variable assignment that the variable is what the type expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die_if_unbearable() value 'hello' violates type hint <class 'int'>, as str 'hello' not instance of int.\n"
     ]
    }
   ],
   "source": [
    "sample_number:int = 10\n",
    "die_if_unbearable(sample_number, int) # should not raise an exception\n",
    "\n",
    "sample_text: int = \"hello\" # doesn't raise an exception, but should\n",
    "try:\n",
    "    die_if_unbearable(sample_text, int) # will raise an exception since the type is not int\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using jaxtyping to validate dtype and shape of tensor/numpy arrays\n",
    "\n",
    "### To start with dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking type for a numpy array, should not raise an exception\n",
    "sample_array: Float32[np.ndarray, \"...\"] = np.array([1, 2, 3], dtype=np.float32)\n",
    "try:\n",
    "    die_if_unbearable(sample_array, Float32[np.ndarray, \"...\"]) # should not raise an exception\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die_if_unbearable() value \"tensor([1., 2., 3.], dtype=torch.float64)\" violates type hint <class 'jaxtyping.Float32[Tensor, '...']'>, as this array has dtype float64, not float32 as expected by the type hint.\n"
     ]
    }
   ],
   "source": [
    "# checking type for a torch tensor, should raise an exception\n",
    "sample_tensor: Float32[torch.Tensor, \"...\"] = torch.tensor([1, 2, 3], dtype=torch.float64)\n",
    "try:\n",
    "    die_if_unbearable(sample_tensor, Float32[torch.Tensor, \"...\"]) # should raise an expection as it is the wrong type\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now tensor shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of correct shape and type for a torch tensor\n",
    "sample_3x3_tensor: Float32[torch.Tensor, \"3 3\"] = torch.rand((3, 3), dtype=torch.float32)\n",
    "try:\n",
    "    die_if_unbearable(sample_3x3_tensor, Float32[torch.Tensor, \"3 3\"]) # should not raise an exception\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die_if_unbearable() value \"tensor([[0.6003, 0.1025, 0.1948, 0.0770],\n",
      "        [0.6433, 0.0538, 0.5310, 0.9717],\n",
      "    ...]])\" violates type hint <class 'jaxtyping.Float32[Tensor, '3 3']'>, as the dimension size 4 does not equal 3 as expected by the type hint.\n"
     ]
    }
   ],
   "source": [
    "# example of incorrect shape but correct type for a torch tensor\n",
    "sample__4x4_tensor: Float32[torch.Tensor, \"3 3\"] = torch.rand((4, 4), dtype=torch.float32)\n",
    "try:\n",
    "    die_if_unbearable(sample__4x4_tensor, Float32[torch.Tensor, \"3 3\"]) # should raise an exception\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type checking function inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type-check error whilst checking the parameters of __main__.add_numbers.\n",
      "The problem arose whilst typechecking parameter 'b'.\n",
      "Actual value: '10'\n",
      "Expected type: <class 'int'>.\n",
      "----------------------\n",
      "Called with parameters: {'a': 5, 'b': '10'}\n",
      "Parameter annotations: (a: int, b: int) -> Any.\n",
      "\n",
      "Type-check error whilst checking the parameters of __main__.process_numbers.\n",
      "----------------------\n",
      "Called with parameters: {'numbers': [1, 2, '3']}\n",
      "Parameter annotations: (numbers: list[int]) -> Any.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "def process_numbers(numbers: list[int]) -> Float32[np.ndarray, \"...\"]:\n",
    "    array:Float32[np.ndarray, \"...\"] = np.array(numbers, dtype=np.float32)\n",
    "    return array\n",
    "\n",
    "# Validate function inputs\n",
    "try:\n",
    "    die_if_unbearable(add_numbers(5, 10), int)  # should not raise an exception\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    die_if_unbearable(add_numbers(5, \"10\"), int)  # should raise an exception\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Validate function outputs\n",
    "try:\n",
    "    result = process_numbers([1, 2, 3])\n",
    "    die_if_unbearable(result, Float32[np.ndarray, \"...\"])  # should not raise an exception\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    result = process_numbers([1, 2, \"3\"])  # should raise an exception\n",
    "    die_if_unbearable(result, Float32[np.ndarray, \"...\"])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Available Datasets\n",
    "With type checking out of the way, lets take a look at lerobot!\n",
    "\n",
    "To explore different available datasets, we can either \n",
    "1. directly look at those provided by lerobot\n",
    "2. Using hfapi\n",
    "3. One can also simple browse [Huggingface LeRobot](https://huggingface.co/datasets?other=LeRobot\n",
    ") directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of available datasets through lerobot: 91\n",
      "\n",
      "First ten available datasets through lerobot:\n",
      "['lerobot/aloha_mobile_cabinet',\n",
      " 'lerobot/aloha_mobile_chair',\n",
      " 'lerobot/aloha_mobile_elevator',\n",
      " 'lerobot/aloha_mobile_shrimp',\n",
      " 'lerobot/aloha_mobile_wash_pan',\n",
      " 'lerobot/aloha_mobile_wipe_wine',\n",
      " 'lerobot/aloha_sim_insertion_human',\n",
      " 'lerobot/aloha_sim_insertion_human_image',\n",
      " 'lerobot/aloha_sim_insertion_scripted',\n",
      " 'lerobot/aloha_sim_insertion_scripted_image']\n",
      "\n",
      "Total number of datasets in the hub with the tag 'LeRobot': 1856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We ported a number of existing datasets ourselves, use this to see the list:\n",
    "available_datasets: list[str] = lerobot.available_datasets\n",
    "die_if_unbearable(available_datasets, list[str])\n",
    "\n",
    "print(F\"Total number of available datasets through lerobot: {len(available_datasets)}\\n\")\n",
    "\n",
    "# print the first ten\n",
    "print(\"First ten available datasets through lerobot:\")\n",
    "pprint(available_datasets[:10])\n",
    "\n",
    "# You can also browse through the datasets created/ported by the community on the hub using the hub api:\n",
    "hub_api = HfApi()\n",
    "repo_ids: list[str] = [info.id for info in hub_api.list_datasets(task_categories=\"robotics\", tags=[\"LeRobot\"])]\n",
    "die_if_unbearable(repo_ids, list[str])\n",
    "\n",
    "print(F\"\\nTotal number of datasets in the hub with the tag 'LeRobot': {len(repo_ids)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking a look at an example dataset\n",
    "I use type annotations extensivly on variable assignments to help me better understand exactly what I'm working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lerobot/aloha_static_cups_open\n",
      "Total number of episodes: 50\n",
      "Average number of frames per episode: 400.000\n",
      "Frames per second used during data collection: 50\n",
      "Robot type: aloha\n",
      "keys to access images from cameras: camera_keys=['observation.images.cam_high', 'observation.images.cam_left_wrist', 'observation.images.cam_low', 'observation.images.cam_right_wrist']\n",
      "\n",
      "LeRobotDatasetMetadata({\n",
      "    Repository ID: 'lerobot/aloha_static_cups_open',\n",
      "    Total episodes: '50',\n",
      "    Total frames: '20000',\n",
      "    Features: '['observation.images.cam_high', 'observation.images.cam_left_wrist', 'observation.images.cam_low', 'observation.images.cam_right_wrist', 'observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.done', 'index', 'task_index']',\n",
      "})',\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_idx:int = 5\n",
    "die_if_unbearable(dataset_idx, int)\n",
    "\n",
    "# repo_id:str = \"lerobot/aloha_static_coffee_new\" #available_datasets[dataset_idx]\n",
    "# repo_id:str =  available_datasets[dataset_idx]\n",
    "repo_id:str = \"lerobot/aloha_static_cups_open\"\n",
    "\n",
    "print(repo_id)\n",
    "\n",
    "die_if_unbearable(repo_id, str)\n",
    "\n",
    "# We can have a look and fetch its metadata to know more about it:\n",
    "ds_meta = LeRobotDatasetMetadata(repo_id)\n",
    "\n",
    "# By instantiating just this class, you can quickly access useful information about the content and the\n",
    "# structure of the dataset without downloading the actual data yet (only metadata files — which are\n",
    "# lightweight).\n",
    "total_episodes: int = ds_meta.total_episodes\n",
    "print(f\"Total number of episodes: {total_episodes}\")\n",
    "avg_frames_per_episode: float = ds_meta.total_frames / total_episodes\n",
    "print(f\"Average number of frames per episode: {avg_frames_per_episode:.3f}\")\n",
    "fps: int = ds_meta.fps\n",
    "print(f\"Frames per second used during data collection: {fps}\")\n",
    "robot_type: str = ds_meta.robot_type\n",
    "print(f\"Robot type: {robot_type}\")\n",
    "camera_keys: list[str] = ds_meta.camera_keys\n",
    "print(f\"keys to access images from cameras: {camera_keys=}\\n\")\n",
    "\n",
    "print(ds_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks:\n",
      "{0: 'Pick up the plastic cup and open its lid.'}\n"
     ]
    }
   ],
   "source": [
    "# look more closely at the tasks, create a new type for the task id and description to be more specific\n",
    "TaskID = NewType('TaskID', int)\n",
    "TaskDescription = NewType('TaskDescription', str)\n",
    "\n",
    "tasks:dict[TaskID, TaskDescription] = ds_meta.tasks\n",
    "die_if_unbearable(tasks, dict[TaskID, TaskDescription])\n",
    "print(f\"Tasks:\\n{tasks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features include things like observations, video, timestamp, ect\n",
    "```\n",
    "├ observation.images.cam_high (VideoFrame):\n",
    "│  │   VideoFrame = {'path': path to a mp4 video, 'timestamp' (float32): timestamp in the video}\n",
    "│  ├ observation.state (list of float32): position of an arm joints (for instance)\n",
    "│  ... (more observations)\n",
    "│  ├ action (list of float32): goal position of an arm joints (for instance)\n",
    "│  ├ episode_index (int64): index of the episode for this sample\n",
    "│  ├ frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode\n",
    "│  ├ timestamp (float32): timestamp in the episode\n",
    "│  ├ next.done (bool): indicates the end of en episode ; True for the last frame in each episode\n",
    "│  └ index (int64): general index in the whole dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "\n",
      "{'action': {'dtype': 'float32',\n",
      "            'names': {'motors': ['left_waist',\n",
      "                                 'left_shoulder',\n",
      "                                 'left_elbow',\n",
      "                                 'left_forearm_roll',\n",
      "                                 'left_wrist_angle',\n",
      "                                 'left_wrist_rotate',\n",
      "                                 'left_gripper',\n",
      "                                 'right_waist',\n",
      "                                 'right_shoulder',\n",
      "                                 'right_elbow',\n",
      "                                 'right_forearm_roll',\n",
      "                                 'right_wrist_angle',\n",
      "                                 'right_wrist_rotate',\n",
      "                                 'right_gripper']},\n",
      "            'shape': (14,)},\n",
      " 'episode_index': {'dtype': 'int64', 'names': None, 'shape': (1,)},\n",
      " 'frame_index': {'dtype': 'int64', 'names': None, 'shape': (1,)},\n",
      " 'index': {'dtype': 'int64', 'names': None, 'shape': (1,)},\n",
      " 'next.done': {'dtype': 'bool', 'names': None, 'shape': (1,)},\n",
      " 'observation.images.cam_high': {'dtype': 'video',\n",
      "                                 'names': ['height', 'width', 'channel'],\n",
      "                                 'shape': (480, 640, 3),\n",
      "                                 'video_info': {'has_audio': False,\n",
      "                                                'video.codec': 'av1',\n",
      "                                                'video.fps': 50.0,\n",
      "                                                'video.is_depth_map': False,\n",
      "                                                'video.pix_fmt': 'yuv420p'}},\n",
      " 'observation.images.cam_left_wrist': {'dtype': 'video',\n",
      "                                       'names': ['height', 'width', 'channel'],\n",
      "                                       'shape': (480, 640, 3),\n",
      "                                       'video_info': {'has_audio': False,\n",
      "                                                      'video.codec': 'av1',\n",
      "                                                      'video.fps': 50.0,\n",
      "                                                      'video.is_depth_map': False,\n",
      "                                                      'video.pix_fmt': 'yuv420p'}},\n",
      " 'observation.images.cam_low': {'dtype': 'video',\n",
      "                                'names': ['height', 'width', 'channel'],\n",
      "                                'shape': (480, 640, 3),\n",
      "                                'video_info': {'has_audio': False,\n",
      "                                               'video.codec': 'av1',\n",
      "                                               'video.fps': 50.0,\n",
      "                                               'video.is_depth_map': False,\n",
      "                                               'video.pix_fmt': 'yuv420p'}},\n",
      " 'observation.images.cam_right_wrist': {'dtype': 'video',\n",
      "                                        'names': ['height', 'width', 'channel'],\n",
      "                                        'shape': (480, 640, 3),\n",
      "                                        'video_info': {'has_audio': False,\n",
      "                                                       'video.codec': 'av1',\n",
      "                                                       'video.fps': 50.0,\n",
      "                                                       'video.is_depth_map': False,\n",
      "                                                       'video.pix_fmt': 'yuv420p'}},\n",
      " 'observation.state': {'dtype': 'float32',\n",
      "                       'names': {'motors': ['left_waist',\n",
      "                                            'left_shoulder',\n",
      "                                            'left_elbow',\n",
      "                                            'left_forearm_roll',\n",
      "                                            'left_wrist_angle',\n",
      "                                            'left_wrist_rotate',\n",
      "                                            'left_gripper',\n",
      "                                            'right_waist',\n",
      "                                            'right_shoulder',\n",
      "                                            'right_elbow',\n",
      "                                            'right_forearm_roll',\n",
      "                                            'right_wrist_angle',\n",
      "                                            'right_wrist_rotate',\n",
      "                                            'right_gripper']},\n",
      "                       'shape': (14,)},\n",
      " 'task_index': {'dtype': 'int64', 'names': None, 'shape': (1,)},\n",
      " 'timestamp': {'dtype': 'float32', 'names': None, 'shape': (1,)}}\n"
     ]
    }
   ],
   "source": [
    "class BaseFeature(TypedDict, total=False):\n",
    "    \"\"\"\n",
    "    For dtype in {\"float32\", \"int64\", \"bool\", \"string\"}.\n",
    "    'shape' is a tuple of ints, 'names' can be None, a list, or a dict.\n",
    "    \"\"\"\n",
    "    dtype: Literal[\"float32\", \"int64\", \"bool\", \"string\"]\n",
    "    shape: tuple[int, ...]\n",
    "    names: list[str] | dict[str, list[str]] | None\n",
    "\n",
    "class VideoFeature(TypedDict, total=False):\n",
    "    \"\"\"\n",
    "    For dtype == \"video\".\n",
    "    Must include 'video_info'.\n",
    "    \"\"\"\n",
    "    dtype: Literal[\"video\"]\n",
    "    shape: tuple[int, ...]\n",
    "    names: list[str] | dict[str, list[str]] | None\n",
    "    video_info: dict[str, Any]\n",
    "\n",
    "\n",
    "Feature = BaseFeature | VideoFeature\n",
    "FeatureName = NewType('FeatureName', str)\n",
    "FeaturesDict = dict[FeatureName, Feature]\n",
    "\n",
    "features:FeaturesDict= ds_meta.features\n",
    "die_if_unbearable(features, FeaturesDict)\n",
    "print(\"Features:\\n\")\n",
    "pprint(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load full dataset, not just metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected episodes: [0, 4, 9]\n",
      "Number of episodes selected: 3\n",
      "Number of frames selected: 1200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fd8578e02844c2baa020b33db554c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes selected: 50\n",
      "Number of frames selected: 20000\n",
      "LeRobotDatasetMetadata({\n",
      "    Repository ID: 'lerobot/aloha_static_cups_open',\n",
      "    Total episodes: '50',\n",
      "    Total frames: '20000',\n",
      "    Features: '['observation.images.cam_high', 'observation.images.cam_left_wrist', 'observation.images.cam_low', 'observation.images.cam_right_wrist', 'observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.done', 'index', 'task_index']',\n",
      "})',\n",
      "\n",
      "Dataset({\n",
      "    features: ['observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.done', 'index', 'task_index'],\n",
      "    num_rows: 20000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# You can then load the actual dataset from the hub.\n",
    "# Either load any subset of episodes:\n",
    "episodes: list[int] = [0, 4, 9]\n",
    "die_if_unbearable(episodes, list[int])\n",
    "dataset = LeRobotDataset(repo_id, episodes=episodes)\n",
    "\n",
    "# And see how many frames you have:\n",
    "print(f\"Selected episodes: {dataset.episodes}\")\n",
    "print(f\"Number of episodes selected: {dataset.num_episodes}\")\n",
    "print(f\"Number of frames selected: {dataset.num_frames}\")\n",
    "\n",
    "# Or simply load the entire dataset:\n",
    "full_dataset = LeRobotDataset(repo_id)\n",
    "print(f\"Number of episodes selected: {full_dataset.num_episodes}\")\n",
    "print(f\"Number of frames selected: {full_dataset.num_frames}\")\n",
    "\n",
    "# The previous metadata class is contained in the 'meta' attribute of the dataset:\n",
    "print(full_dataset.meta)\n",
    "\n",
    "# LeRobotDataset actually wraps an underlying Hugging Face dataset\n",
    "# (see https://huggingface.co/docs/datasets for more information).\n",
    "print(full_dataset.hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 starts at frame 0 and ends at frame 400\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 480, 640])\n",
      "torch.float32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca374a3ac962474c8920a550c1abe63e",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Viewer()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LeRobot datasets also subclasses PyTorch datasets so you can do everything you know and love from working\n",
    "# with the latter, like iterating through the dataset.\n",
    "# The __getitem__ iterates over the frames of the dataset. Since our datasets are also structured by\n",
    "# episodes, you can access the frame indices of any episode using the episode_data_index. Here, we access\n",
    "# frame indices associated to the first episode:\n",
    "episode_index:int = 0\n",
    "die_if_unbearable(episode_index, int)\n",
    "\n",
    "from_idx:int = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "to_idx:int = dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "print(f\"Episode {episode_index} starts at frame {from_idx} and ends at frame {to_idx}\")\n",
    "# # Then we grab all the image frames from the first camera:\n",
    "camera_key: str = dataset.meta.camera_keys[0]\n",
    "die_if_unbearable(camera_key, str)\n",
    "\n",
    "# loading all frames like this can take a while, so only do the first 10 frames\n",
    "frames:list[Float32[torch.Tensor, \"3 H W\"]] = [dataset[idx][camera_key] for idx in range(from_idx, 150)]\n",
    "die_if_unbearable(frames, list[Float32[torch.Tensor, \"3 H W\"]])\n",
    "\n",
    "# The show that the frames are float32 tensors with shape (3, H, W) or in this case (3, 480, 640)\n",
    "print(type(frames[0]))\n",
    "print(frames[0].shape)\n",
    "print(frames[0].dtype)\n",
    "\n",
    "# lets visualize these images\n",
    "\n",
    "rr.init(\"lerobot images\")\n",
    "for idx, frame in enumerate(frames):\n",
    "    rr.set_time_sequence(\"frame_idx\", idx)\n",
    "    # convert to h w 3\n",
    "    rgb_tensor:Float32[torch.Tensor, \"H W 3\"] = rearrange(frame, 'C H W -> H W C')\n",
    "    die_if_unbearable(rgb_tensor, Float32[torch.Tensor, \"H W 3\"])\n",
    "\n",
    "    rgb_array:Float32[np.ndarray, \"H W 3\"] = rgb_tensor.numpy(force=True)\n",
    "    die_if_unbearable(rgb_array, Float32[np.ndarray, \"H W 3\"])\n",
    "\n",
    "    # convert from 0-1 to 0-255 and convert to uint8\n",
    "    rgb_array:UInt8[np.ndarray, \"H W 3\"] = (rgb_array * 255).astype(np.uint8)\n",
    "    die_if_unbearable(rgb_array, UInt8[np.ndarray, \"H W 3\"])\n",
    "\n",
    "    rr.log(\"image\", rr.Image(rgb_array).compress(jpeg_quality=70))\n",
    "\n",
    "rr.notebook_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For many machine learning applications we need to load the history of past observations or trajectories of\n",
    "# future actions. Our datasets can load previous and future frames for each key/modality, using timestamps\n",
    "# differences with the current loaded frame. For instance:\n",
    "delta_timestamps:dict[str, list[float | int]] = {\n",
    "    # loads 4 images: 1 second before current frame, 500 ms before, 200 ms before, and current frame\n",
    "    camera_key: [-1, -0.5, -0.20, 0],\n",
    "    # loads 8 state vectors: 1.5 seconds before, 1 second before, ... 200 ms, 100 ms, and current frame\n",
    "    \"observation.state\": [-1.5, -1, -0.5, -0.20, -0.10, 0],\n",
    "    # loads 64 action vectors: current frame, 1 frame in the future, 2 frames, ... 63 frames in the future\n",
    "    \"action\": [t / dataset.fps for t in range(64)],\n",
    "}\n",
    "die_if_unbearable(delta_timestamps, dict[str, list[float | int]])\n",
    "# Note that in any case, these delta_timestamps values need to be multiples of (1/fps) so that added to any\n",
    "# timestamp, you still get a valid timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99993b0c70314d42a4251ee1498fb900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "delta_frames.shape=torch.Size([4, 3, 480, 640])\n",
      "delta_states.shape=torch.Size([6, 14])\n",
      "delta_actions.shape=torch.Size([64, 14])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = LeRobotDataset(repo_id, delta_timestamps=delta_timestamps)\n",
    "\n",
    "delta_frames:Float32[torch.Tensor, \"4 3 H W\"] = dataset[0][camera_key]\n",
    "die_if_unbearable(delta_frames, Float32[torch.Tensor, \"4 3 H W\"])\n",
    "\n",
    "# positsions of arm joints and\n",
    "delta_states:Float32[torch.Tensor, \"6 num_motors\"] = dataset[0][\"observation.state\"]\n",
    "die_if_unbearable(delta_states, Float32[torch.Tensor, \"6 num_motors\"])\n",
    "\n",
    "# actions to be taken\n",
    "delta_actions:Float32[torch.Tensor, \"64 num_motors\"] = dataset[0][\"action\"]\n",
    "die_if_unbearable(delta_actions, Float32[torch.Tensor, \"64 num_motors\"])\n",
    "\n",
    "\n",
    "print(f\"\\n{delta_frames.shape=}\")  # (4, c, h, w)\n",
    "print(f\"{delta_states.shape=}\")  # (6, num_motors)\n",
    "print(f\"{delta_actions.shape=}\\n\")  # (64, num_motors)\n",
    "\n",
    "# Finally, our datasets are fully compatible with PyTorch dataloaders and samplers because they are just\n",
    "# PyTorch datasets.\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=0,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "    batch_frames:Float32[torch.Tensor, \"32 4 3 H W\"] = batch[camera_key]\n",
    "    die_if_unbearable(batch_frames, Float32[torch.Tensor, \"32 4 3 H W\"])\n",
    "\n",
    "    batch_states:Float32[torch.Tensor, \"32 6 num_motors\"] = batch[\"observation.state\"]\n",
    "    die_if_unbearable(batch_states, Float32[torch.Tensor, \"32 6 num_motors\"])\n",
    "\n",
    "    batch_actions:Float32[torch.Tensor, \"32 64 num_motors\"] = batch[\"action\"]\n",
    "    die_if_unbearable(batch_actions, Float32[torch.Tensor, \"32 64 num_motors\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally lets visualize the dataset using rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67db13cffd04aac9384a4b93033757d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e090ad0df34d79b3870ae6e7a03ab2",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Viewer()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-11T00:14:20Z ERROR re_log::result_extensions] rerun_py/src/python_bridge.rs:835 ZMQError: Too many open files                                                                                      | 1/13 [00:03<00:45,  3.77s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:45<00:00,  3.46s/it]\n"
     ]
    }
   ],
   "source": [
    "import rerun.blueprint as rrb\n",
    "import tqdm\n",
    "from jaxtyping import Int64\n",
    "from lerobot.scripts.visualize_dataset import EpisodeSampler\n",
    "\n",
    "rr.init(\"Final notebook visualization\")\n",
    "\n",
    "dataset = LeRobotDataset(repo_id)\n",
    "\n",
    "episode_index:int = 0\n",
    "episode_sampler = EpisodeSampler(dataset, episode_index)\n",
    "\n",
    "batch_size:int = 32\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=0,\n",
    "    batch_size=batch_size,\n",
    "    sampler=episode_sampler,\n",
    ")\n",
    "\n",
    "blueprint =rrb.Blueprint(\n",
    "    rrb.Vertical(\n",
    "        rrb.Grid(contents=[rrb.Spatial2DView(origin=key) for key in dataset.meta.camera_keys]),\n",
    "        rrb.TimeSeriesView(),\n",
    "    ),\n",
    "    collapse_panels=True\n",
    ")\n",
    "rr.log(\"test\", rr.Scalar(0.0))\n",
    "rr.notebook_show(blueprint=blueprint, height=500,width=1000)\n",
    "for batch in tqdm.tqdm(dataloader, total=len(dataloader)):\n",
    "    # iterate over the batch\n",
    "    batch_index:Int64[torch.Tensor, \"batch_size\"] = batch[\"index\"]\n",
    "    die_if_unbearable(batch_index, Int64[torch.Tensor, \"batch_size\"])\n",
    "    for i in range(len(batch_index)):\n",
    "        frame_index:int = batch[\"frame_index\"][i].item()\n",
    "        die_if_unbearable(frame_index, int)\n",
    "        rr.set_time_sequence(\"frame_index\", frame_index)\n",
    "\n",
    "        timestamp:float = batch[\"timestamp\"][i].item()\n",
    "        die_if_unbearable(timestamp, float)\n",
    "        rr.set_time_seconds(\"timestamp\", timestamp)\n",
    "    \n",
    "        # display each camera image\n",
    "        for key in dataset.meta.camera_keys:\n",
    "            # convert from tensor format to numpy\n",
    "            rgb_tensor:Float32[torch.Tensor, \"3 H W\"] = batch[key][i]\n",
    "            rgb_array:Float32[np.ndarray, \"H W 3\"] = rearrange(rgb_tensor, 'C H W -> H W C').numpy(force=True)\n",
    "            rgb_array:UInt8[np.ndarray, \"H W 3\"] = (rgb_array * 255).astype(np.uint8)\n",
    "            rr.log(\n",
    "                key,\n",
    "                rr.Image(rgb_array).compress(\n",
    "                    jpeg_quality=95\n",
    "                ),\n",
    "            )\n",
    "    \n",
    "        # display each dimension of action space (e.g. actuators command)\n",
    "        if \"action\" in batch:\n",
    "            for dim_idx, val in enumerate(batch[\"action\"][i]):\n",
    "                rr.log(f\"action/{dim_idx}\", rr.Scalar(val.item()))\n",
    "    \n",
    "        # display each dimension of observed state space (e.g. agent position in joint space)\n",
    "        if \"observation.state\" in batch:\n",
    "            for dim_idx, val in enumerate(batch[\"observation.state\"][i]):\n",
    "                rr.log(f\"state/{dim_idx}\", rr.Scalar(val.item()))\n",
    "    \n",
    "        if \"next.done\" in batch:\n",
    "            rr.log(\"next.done\", rr.Scalar(batch[\"next.done\"][i].item()))\n",
    "    \n",
    "        if \"next.reward\" in batch:\n",
    "            rr.log(\"next.reward\", rr.Scalar(batch[\"next.reward\"][i].item()))\n",
    "    \n",
    "        if \"next.success\" in batch:\n",
    "            rr.log(\"next.success\", rr.Scalar(batch[\"next.success\"][i].item()))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
