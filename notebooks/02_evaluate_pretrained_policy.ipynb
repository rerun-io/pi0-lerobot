{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Literal, NewType, TypedDict\n",
    "\n",
    "import gym_pusht  # noqa: F401\n",
    "import gymnasium as gym\n",
    "\n",
    "# for performing runtime typechking in a iPython environment.\n",
    "import jaxtyping\n",
    "import numpy as np\n",
    "import rerun as rr\n",
    "import torch\n",
    "from beartype.door import die_if_unbearable\n",
    "from lerobot.common.policies.diffusion.modeling_diffusion import DiffusionPolicy\n",
    "\n",
    "%load_ext jaxtyping\n",
    "%jaxtyping.typechecker beartype.beartype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load in pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store the video of the evaluation\n",
    "output_directory = Path(\"outputs/eval/example_pusht_diffusion\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Select your device\n",
    "device = \"cuda\"\n",
    "\n",
    "# Provide the [hugging face repo id](https://huggingface.co/lerobot/diffusion_pusht):\n",
    "pretrained_policy_path = \"lerobot/diffusion_pusht\"\n",
    "# OR a path to a local outputs/train folder.\n",
    "# pretrained_policy_path = Path(\"outputs/train/example_pusht_diffusion\")\n",
    "\n",
    "policy = DiffusionPolicy.from_pretrained(pretrained_policy_path, map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize eval envrionment\n",
    "this provides an image of the scene, and the state/position of the agent\n",
    "will stop running after 300 interactions/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs\n",
      "Dict('agent_pos': Box(0.0, 512.0, (2,), float64), 'pixels': Box(0, 255, (96, 96, 3), uint8))\n",
      "{'observation.image': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 96, 96)), 'observation.state': PolicyFeature(type=<FeatureType.STATE: 'STATE'>, shape=(2,))}\n",
      "\n",
      "Individual Inputs Seperated out\n",
      "Box(0.0, 512.0, (2,), float64)\n",
      "PolicyFeature(type=<FeatureType.STATE: 'STATE'>, shape=(2,))\n",
      "Box(0, 255, (96, 96, 3), uint8)\n",
      "PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 96, 96))\n",
      "\n",
      "Outputs\n",
      "Box(0.0, 512.0, (2,), float32)\n",
      "{'action': PolicyFeature(type=<FeatureType.ACTION: 'ACTION'>, shape=(2,))}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"gym_pusht/PushT-v0\",\n",
    "    obs_type=\"pixels_agent_pos\",\n",
    "    max_episode_steps=300,\n",
    ")\n",
    "print(\"Inputs\")\n",
    "print(env.observation_space)\n",
    "print(policy.config.input_features)\n",
    "print()\n",
    "\n",
    "print(\"Individual Inputs Seperated out\")\n",
    "print(env.observation_space[\"agent_pos\"])\n",
    "print(policy.config.input_features[\"observation.state\"])\n",
    "print(env.observation_space[\"pixels\"])\n",
    "print(policy.config.input_features[\"observation.image\"])\n",
    "print()\n",
    "\n",
    "print(\"Outputs\")\n",
    "print(env.action_space)\n",
    "print(policy.config.output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636df2ed62794f888ffaf63dc5c6c8d9",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Viewer()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "from jaxtyping import Float32, Float64, UInt8\n",
    "from torch._tensor import Tensor\n",
    "\n",
    "# Reset the policy and environments to prepare for rollout\n",
    "policy.reset()\n",
    "numpy_observation, info = env.reset(seed=42)\n",
    "die_if_unbearable(numpy_observation, dict[str, np.ndarray])\n",
    "\n",
    "rr.init(\"evaluate_policy\")\n",
    "\n",
    "# Prepare to collect every rewards and all the frames of the episode,\n",
    "# from initial state to final state.\n",
    "rewards = []\n",
    "# frames = []\n",
    "\n",
    "# Render frame of the initial state\n",
    "# frames.append(env.render())\n",
    "rr.set_time_sequence(\"step\", 0)\n",
    "rr.log(\"frame\", rr.Image(env.render()).compress(jpeg_quality=70))\n",
    "rr.notebook_show(width=1200)\n",
    "\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "while not done:\n",
    "    rr.set_time_sequence(\"step\", step)\n",
    "\n",
    "    # Prepare observation for the policy running in Pytorch\n",
    "    state:Float64[torch.Tensor, \"2\"] = torch.from_numpy(numpy_observation[\"agent_pos\"])\n",
    "    image:UInt8[torch.Tensor, \"96 96 3\"] = torch.from_numpy(numpy_observation[\"pixels\"])\n",
    "    die_if_unbearable(state, Float64[torch.Tensor, \"2\"])\n",
    "    die_if_unbearable(image, UInt8[torch.Tensor, \"96 96 3\"])\n",
    "\n",
    "    # Convert to float32 with image from channel first in [0,255]\n",
    "    # to channel last in [0,1]\n",
    "    state:Float32[torch.Tensor, \"2\"] = state.to(torch.float32)\n",
    "    die_if_unbearable(state, Float32[torch.Tensor, \"2\"])\n",
    "\n",
    "    image:Float32[torch.Tensor, \"96 96 3\"] = image.to(torch.float32) / 255\n",
    "    image:Float32[torch.Tensor, \"3 96 96\"]  = rearrange(image, \"h w c -> c h w\")\n",
    "    die_if_unbearable(image, Float32[torch.Tensor, \"3 96 96\"])\n",
    "\n",
    "    # Send data tensors from CPU to GPU\n",
    "    state = state.to(device, non_blocking=True)\n",
    "    image = image.to(device, non_blocking=True)\n",
    "\n",
    "    # Add extra (empty) batch dimension, required to forward the policy\n",
    "    state = rearrange(state, 'd -> 1 d')\n",
    "    image = rearrange(image, 'c h w -> 1 c h w')\n",
    "    die_if_unbearable(state, Float32[torch.Tensor, \"1 2\"])\n",
    "    die_if_unbearable(image, Float32[torch.Tensor, \"1 3 96 96\"])\n",
    "\n",
    "    # Create the policy input dictionary\n",
    "    observation: dict[str, Tensor] = {\n",
    "        \"observation.state\": state,\n",
    "        \"observation.image\": image,\n",
    "    }\n",
    "\n",
    "    # Predict the next action with respect to the current observation\n",
    "    with torch.inference_mode():\n",
    "        action:Float32[torch.Tensor, \"1 2\"] = policy.select_action(observation)\n",
    "        die_if_unbearable(action, Float32[torch.Tensor, \"1 2\"])\n",
    "\n",
    "    # Prepare the action for the environment\n",
    "    numpy_action:Float32[np.ndarray, \"2\"] = rearrange(action, '1 d -> d').numpy(force=True)\n",
    "    die_if_unbearable(numpy_action, Float32[np.ndarray, \"2\"])\n",
    "\n",
    "    # Step through the environment and receive a new observation\n",
    "    env_step = env.step(numpy_action)\n",
    "    # extract the observation, reward, done, info from the environment step\n",
    "    numpy_observation: dict[str, np.ndarray] = env_step[0]\n",
    "    die_if_unbearable(numpy_observation, dict[str, np.ndarray])\n",
    "\n",
    "    reward: float = env_step[1]\n",
    "    die_if_unbearable(reward, float)\n",
    "\n",
    "    terminated: bool = env_step[2]\n",
    "    die_if_unbearable(terminated, bool)\n",
    "\n",
    "    truncated: bool = env_step[3]\n",
    "    die_if_unbearable(truncated, bool)\n",
    "\n",
    "    info: dict[str, Any] = env_step[4]\n",
    "    die_if_unbearable(info, dict[str, Any])\n",
    "\n",
    "    # print(f\"{step=} {reward=} {terminated=}\")\n",
    "\n",
    "    # Keep track of all the rewards and frames\n",
    "    rewards.append(reward)\n",
    "    # frames.append(env.render())\n",
    "    rr.log(\"frame\", rr.Image(env.render()).compress(jpeg_quality=70))\n",
    "    rr.log(\"reward\", rr.Scalar(reward))\n",
    "\n",
    "    # The rollout is considered done when the success state is reach (i.e. terminated is True),\n",
    "    # or the maximum number of iterations is reached (i.e. truncated is True)\n",
    "    done = terminated | truncated | done\n",
    "    step += 1\n",
    "\n",
    "if terminated:\n",
    "    print(\"Success!\")\n",
    "else:\n",
    "    print(\"Failure!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
